# Abstract:
The project's main objective is to analyze the tweeter users' sentiments related to the US 2020 presidential elections and construct a model that can accurately predict results of the election by states. This will be done by using Word2Vec from the gensim Python library. To be specific, the Continuous Bag of Words (CBOW) model will be used in order to get a deeper insight into the sentiments associated with selected political issues and actors. We will then assess its efficiency by computing the error rate between the model’s predictions and the actual results, by states.

# Introduction:
The domestic political climate in the US during the 2020 election was highly tense due to several key factors such as the COVID-19 pandemic (Baker et al., 2020), the economic recession (Coibion et al., 2020) and the social unrest that followed the police brutality incidents (Rainey, 2020). The 2020 US presidential election was one of the most polarized in the recent past, with incumbent President Donald Trump facing the challenger Joe Biden, the former vice president. The election was also characterized by high levels of absentee and mail-in voting due to the pandemic and widespread doubts about the election's legitimacy.

Inspired by the research by Alvi et al. published in 2016, where the authors analyzed the tweets to predict the outcome of the US presidential elections, the goal of this work is to apply this method to the 2020 US elections. Since one of the most popular social media that people used to share their opinions during the election period is Twitter, we are going to look at the sentiments that people shared on this platform. Our main research question is: “Is it possible to use sentiment analysis of the posts to forecast the election results in different states of the US?” The research may help to understand how social media can reflect public opinion and attitudes toward key political figures and parties during campaigns.

# Methodology:
## Data:
The data is retrieved from [Kaggle](https://www.kaggle.com/datasets/manchunhui/us-election-2020-tweets).
The brief summary of the dataset by the author: "Tweets collected, using the Twitter API **statuses_lookup** and **snsscrape** for keywords, with the original intention to try to update this dataset daily so that the timeframe will eventually cover 15.10.2020 and 04.11.2020. **Added 06.11.2020** With the events of the election still ongoing as of the date that this comment was added, I've decided to keep updating the dataset with tweets until at least the end of the 6th Nov. **Added 08.11.2020**, just one more version pending to include tweets until at the end of the 8th Nov".
The parsing was done using the hashtags that were relevant to political discourse in 2020 including _#DonaldTrump, #Trump_ for Trump and _#JoeBiden, #Biden_ for Biden. The dataset consists of different information the main of which include: textual information (the tweets themselves), likes and retweets of the tweet, the timestamp of the tweet, and the country, state, and the city of the user who wrote the tweet. The retrieved data is contained in two separate csv files, one for Trump called _hashtag_donaldtrump.csv_, and the other for Biden called _hashtag_joebiden.csv_.

## Data preprocessing:
We started by removing non-English tweets, as our methods are tailored for analyzing English text. From the remaining data, we filtered out tweets that are not from the US, since our focus is on the US elections, with specific analysis based on data from individual states. We also excluded tweets with fewer than 10 characters, as such posts are often do not contain any meaningful information. Then, we kept only the relevant columns, such as the candidate (based on hashtags), tweet post date, state, and the tweet itself. This process was applied separately to the Trump and Biden datasets. Once the individual data cleaning for the two datasets was complete, we concatenated the two datasets.

## Word2Vec configuration
The model size was set to 100, and the window parameter to 10, meaning that the resulting word vectors had 100 dimensions, and the model read words within a window of 10 words. We “fed” the lists of positive and negative words to the model to construct positive and negative vectors. The final positivity vector was created by taking the difference between the two, which served as our benchmark for comparison.
Each word vector (tweet) was then compared to the positivity vector, with the sentiment score increasing the closer the word vector was to the positivity vector, and decreasing the further away it was. We used cosine similarity as the distance measurement.
Once we had our sentiment scores, we normalized them using min-max normalization to get a normalized sentiment score for each candidate. We plotted the density plots for each candidate’s sentiment score to identify the distribution of the scores. Additionally, we analyzed the average sentiment score by candidate over time to assess if there was any strong anomaly in the data. We then generated choropleth maps that showed the 1) general average sentiment score for each US state and 2) average sentiment score for each US state by candidate. According to our method, the candidate with the higher average sentiment score in a given state was predicted to win that state.


## What could also be done?
### Research Design and Model Validation
Given the time restrictions and the course project requirements, integrating more sophisticated methods would be inefficient and excessive as they are beyond the scope of the course. However, it is worth noting that there are several limitations that could be handled.
We could validate the data using the polling data. If polling data is available, we could compare the sentiment scores with poll results at the state level. By calculating Pearson/Spearman correlation coefficients between sentiment scores and polling numbers we could validate the research design in general. If the correlation is low it would indicate either that 1) Twitter data is actually biased, 2) the model is not performing very well. To test the model itself, we can perform independent human-coding analyzing the sentiment scores from -1 to 1 of random twitter sentences. If there's high correlation, it indicates that the model's scores align with public sentiment trends as captured by polls.

### Comparing performance of the created Word2Vec model with the [VADER](https://github.com/cjhutto/vaderSentiment) model
"VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media". We believe comparing the results of our model with the VADER model that specializes in analyzing sentiments of social media users is worth considering. This VADER is trained on larger amount of data it is highly probable that it would be more precise. Therefore, using it as a sort of a benchmark makes sense.  

